{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NLP_Final_FRIENDS_ELECTRA.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oTPKFSN8HSIr"},"source":["\n","# Pytorch를 활용한 미국드라마 프렌즈 대사 감정 분석 모델 \n","## 모델 : ELECTRA Base\n","\n","## Dataset\n","EmotionLines 제공 프렌즈 대사 데이터셋<br>\n","\n","## References\n","- https://github.com/jiwonny/nlp_emotion_classification/blob/master/friends_electra.ipynb (소스코드 참조)\n","- http://doraemon.iis.sinica.edu.tw/emotionlines/index.html\n","- https://huggingface.co/transformers/training.html\n","- http://wikidocs.net/book/2155 (데이터 구조 분석 부분 참조)\n","\n","## 개발 환경\n","  - Google Corab (With GPU)<br>\n","  - 구글 드라이브 연동 후 본인 경로 설정 필수<br>\n"]},{"cell_type":"code","metadata":{"id":"KBn1pCOKk7k1"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUGpGUwOdPoz","executionInfo":{"status":"ok","timestamp":1608737468844,"user_tz":-540,"elapsed":3361,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"ef0b24e6-5b35-4038-bce2-41d9c05684ff"},"source":["#구글 코랩 환경에서 필수 다운로드 요소인 'Transformers' 불러오기\n","!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TWk7PSmTwcUj"},"source":["# Git 연동 작업하기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNzq2E9UJEge","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608743311212,"user_tz":-540,"elapsed":19651,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"09113131-3655-4f1c-ea96-4adb60aaf3f5"},"source":["# 구글 드라이브 마운트하기\r\n","\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"28LIj5DOdm1L"},"source":["# 모델 생성에 필요한 각종 필수 도구 Import\n","\n","import torch\n","\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhRtG-m7dy6J"},"source":["# 판다스를 활용한 Json 데이터 프레임 생성 함수 설정\n","\n","def jsonToDf(file_name):\n","  with open(file_name, encoding = 'utf-8', mode = 'r') as file:\n","    json_array = json.load(file)\n","  result = pd.DataFrame.from_dict(json_array[0])\n","  is_first = True\n","  for array in json_array:\n","      \n","    if is_first:\n","        is_first = False\n","        continue\n","    \n","    temp_df = pd.DataFrame.from_dict(array)\n","    result = result.append(temp_df, ignore_index = True)\n","\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvtRZtrnea9m"},"source":["# Json 데이터 불러오기  (본인 경로 설정 필요)\n","train = jsonToDf('/content/drive/MyDrive/Colab Notebooks/Korea Univ/001. NLP Project/Freinds_eng/friends_train.json')\n","dev = jsonToDf('/content/drive/MyDrive/Colab Notebooks/Korea Univ/001. NLP Project/Freinds_eng/friends_dev.json')\n","test = jsonToDf('/content/drive/MyDrive/Colab Notebooks/Korea Univ/001. NLP Project/Freinds_eng/friends_test.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5lZie1BfeTh"},"source":["# 모델 평가용 데이터 불러오기 (본인 경로 설정 필요)\r\n","predict = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Korea Univ/001. NLP Project/Freinds_eng/en_data.csv', encoding = 'UTF-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNAkmnaPfkOk","executionInfo":{"status":"ok","timestamp":1608737470265,"user_tz":-540,"elapsed":4754,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"7eb305d4-186d-40e0-bd66-af5d3d20cbca"},"source":["# Train Data 파악하기\n","train.utterance\n","max(len(l) for l in train.utterance)\n","sum(map(len, train.utterance))/len(train.utterance)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39.68478363791308"]},"metadata":{"tags":[]},"execution_count":316}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4w60HU496owA","executionInfo":{"status":"ok","timestamp":1608737470266,"user_tz":-540,"elapsed":4750,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"ece0ae00-2f6a-4d4a-bd4b-778aa61a7565"},"source":["#Train Data 중 utterance Data 관찰\r\n","train.utterance"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        also I was the point person on my companys tr...\n","1                         You mustve had your hands full.\n","2                                  That I did. That I did.\n","3            So lets talk a little bit about your duties.\n","4                                   My duties?  All right.\n","                               ...                        \n","10556                                           You or me?\n","10557    I got it. Uh, Joey, women don't have Adam's ap...\n","10558                 You guys are messing with me, right?\n","10559                                                Yeah.\n","10560    That was a good one. For a second there, I was...\n","Name: utterance, Length: 10561, dtype: object"]},"metadata":{"tags":[]},"execution_count":317}]},{"cell_type":"code","metadata":{"id":"ei_GQ3Wp7Dzx"},"source":["# Traint Data의 비율 분석을 위한 세팅\n","def below_threshold_len(max_len, nested_list):\n","  cnt = 0\n","  for s in nested_list:\n","    if(len(s) <= max_len):\n","      cnt = cnt +1\n","  print('전체 대본 중 길이가 %s 이하인 데이터셋의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_hKjJfG7t8b","executionInfo":{"status":"ok","timestamp":1608738010295,"user_tz":-540,"elapsed":960,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"9170b6a7-f6f7-444b-95fb-b237afd08167"},"source":["# Train Data 비율 분석 진행\n","MAX_LEN = 100\n","below_threshold_len(MAX_LEN, train.utterance)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["전체 대본 중 길이가 100 이하인 데이터셋의 비율: 94.59331502698608\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOz9HiP6hoEr","executionInfo":{"status":"ok","timestamp":1608737470267,"user_tz":-540,"elapsed":4736,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"3c2629f6-fb38-4738-ba11-49aa51db9e9f"},"source":["# 전체 데이터셋 수치 파악\n","print(train.shape)\n","print(dev.shape)\n","print(test.shape)\n","print(predict.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(10561, 4)\n","(1178, 4)\n","(2764, 4)\n","(1623, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QVXA6vh8uVC9"},"source":["# Train, Dev, Test, Predict 데이터 전처리 과정"]},{"cell_type":"code","metadata":{"id":"e7A-PmRkh5o7"},"source":["# Train,Dev,Test 데이터의 전처리 진행을 위한 함수 설정\n","# Max Len은 전체 데이터셋의 95%를 수준을 커버하는 100으로 설정\n","\n","MAX_LEN = MAX_LEN\n","\n","def getInputsAndLabels(dataset):\n","  data = dataset.copy(deep=True)\n","  data['utterance'] = data['utterance'].str.lower()\n","\n","  utterances = data['utterance']\n","  utterances = [\"[CLS] \" + str(utterance) + \" [SEP]\" for utterance in utterances]\n","  \n","  encoder = LabelEncoder()\n","  labels = data['emotion'].values\n","  encoder.fit(labels)\n","  labels = encoder.transform(labels)\n","\n","  tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n","  tokenized_texts = [tokenizer.tokenize(utterance) for utterance in utterances]\n","\n","  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","  attention_masks = []\n","  for seq in input_ids:\n","      seq_mask = [float(i>0) for i in seq]\n","      attention_masks.append(seq_mask)\n","\n","  return input_ids, labels, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"flFWbBykia6M"},"source":["# Predict Data 전처리를 위한 함수 설정 \n","def getInputsFromTest(dataset):\n","  data = dataset.copy(deep=True)\n","  data['utterance'] = data['utterance'].str.lower()\n","\n","  utterances = data['utterance']\n","  utterances = [\"[CLS] \" + str(utterance) + \" [SEP]\" for utterance in utterances]\n","  \n","  tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n","  tokenized_texts = [tokenizer.tokenize(utterance) for utterance in utterances]\n","\n","  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","  attention_masks = []\n","  for seq in input_ids:\n","      seq_mask = [float(i>0) for i in seq]\n","      attention_masks.append(seq_mask)\n","\n","  return input_ids, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D57F-6vWieSU"},"source":["def getIndex(dataset):\n","  data = dataset.copy(deep = True)\n","  input_index = data.id.tolist()\n","  return torch.tensor(input_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJKhnt0zih-Y"},"source":["# Train, Dev, Test 및 Predict 각 각의 데이터에 맞는 전처리 함수 적용\n","\n","train_inputs, train_labels, train_masks = getInputsAndLabels(train)\n","dev_inputs, dev_labels, dev_masks = getInputsAndLabels(dev)\n","test_inputs, test_labels, test_masks = getInputsAndLabels(test)\n","predict_inputs, predict_masks = getInputsFromTest(predict)   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwvfhKSyinB2"},"source":["# 파이토치 텐세로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","\n","dev_inputs = torch.tensor(dev_inputs)\n","dev_labels = torch.tensor(dev_labels)\n","dev_masks = torch.tensor(dev_masks)\n","\n","test_inputs = torch.tensor(test_inputs)\n","test_labels = torch.tensor(test_labels)\n","test_masks = torch.tensor(test_masks)\n","\n","predict_index = getIndex(predict)     \n","predict_inputs = torch.tensor(predict_inputs)\n","predict_masks = torch.tensor(predict_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSAgXWlWzjRH","executionInfo":{"status":"ok","timestamp":1608737476398,"user_tz":-540,"elapsed":10848,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"bfa4fbd1-3f25-4ff3-fecc-4bc69952c666"},"source":["train_inputs\n","dev_inputs\n","test_inputs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  101,  2339,  2079,  ...,     0,     0,     0],\n","        [  101,  2821,  1012,  ...,     0,     0,     0],\n","        [  101,  1061,  1005,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  3398,  2008,  ...,     0,     0,     0],\n","        [  101, 13814,  2054,  ...,     0,     0,     0],\n","        [  101,  2035,  2157,  ...,     0,     0,     0]])"]},"metadata":{"tags":[]},"execution_count":326}]},{"cell_type":"code","metadata":{"id":"_uWhZ8_nitFG"},"source":["# 파이토치의 DataLoader로 입력\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","\n","batch_size = 32\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","dev_data = TensorDataset(dev_inputs, dev_masks, dev_labels)\n","dev_sampler = SequentialSampler(dev_data)\n","dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n","\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","\n","\n","predict_data = TensorDataset(predict_index, predict_inputs, predict_masks) \n","predict_sampler = RandomSampler(predict_data)\n","predict_dataloader = DataLoader(predict_data, sampler=predict_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkQKK7UVmFp8","executionInfo":{"status":"ok","timestamp":1608737476399,"user_tz":-540,"elapsed":10841,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"0461f997-20fc-4825-be07-59e3f3aa50ee"},"source":["# 디바이스 설정 (GPU 활성화 확인)\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q284YDxGmIya","executionInfo":{"status":"ok","timestamp":1608737480824,"user_tz":-540,"elapsed":15261,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"46868dfb-c8fb-4e94-ec2e-a798053ea9b9"},"source":["# 모델 불러오기 (분류 모델)\n","model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=8)\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":329}]},{"cell_type":"markdown","metadata":{"id":"1xyjVMNOgT0k"},"source":["# Train & Validation"]},{"cell_type":"code","metadata":{"id":"n4ZiYZEFm3_s"},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, #학습률\n","                  eps = 1e-8 # 0으로 나누는 것 방지\n","                )\n","epochs = 5\n","\n","# 총 훈련 스텝 = 배치 반복 횟수 * epochs\n","total_steps = len(train_dataloader) * epochs\n","\n","# 학습률을 조금씩 감소시키는 스케줄러 생성\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuR9LZktm_Vm"},"source":["from sklearn.metrics import f1_score\n","\n","# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpxVeaU4nKiT"},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chB2oD0mnRre","executionInfo":{"status":"ok","timestamp":1608737802343,"user_tz":-540,"elapsed":336766,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"9953adc7-27f0-4211-f86a-9976f60d45e6"},"source":["# 재현을 위한 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화 설정\n","\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    t0 = time.time()\n","    total_loss = 0\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        if step % 300 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        batch = tuple(t.to(device).long() for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","             \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","\n","        loss = outputs[0]\n","        total_loss += loss.item()\n","\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    model.eval()\n","    eval_loss, eval_accuracy, eval_f1 = 0, 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in dev_dataloader:\n","        batch = tuple(t.to(device).long() for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        with torch.no_grad():     \n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        logits = outputs[0]\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","     \n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","   \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch   300  of    331.    Elapsed: 0:01:11.\n","\n","  Average training loss: 1.38\n","  Training epcoh took: 0:01:18\n","\n","Running Validation...\n","  Accuracy: 0.55\n","  Validation took: 0:00:02\n","\n","Training complete!\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch   300  of    331.    Elapsed: 0:01:11.\n","\n","  Average training loss: 1.11\n","  Training epcoh took: 0:01:18\n","\n","Running Validation...\n","  Accuracy: 0.57\n","  Validation took: 0:00:02\n","\n","Training complete!\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch   300  of    331.    Elapsed: 0:01:11.\n","\n","  Average training loss: 0.97\n","  Training epcoh took: 0:01:18\n","\n","Running Validation...\n","  Accuracy: 0.59\n","  Validation took: 0:00:02\n","\n","Training complete!\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch   300  of    331.    Elapsed: 0:01:11.\n","\n","  Average training loss: 0.88\n","  Training epcoh took: 0:01:18\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation took: 0:00:02\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3xpnsNf_gYf6"},"source":["# Testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QqDZYh1jeW1n","executionInfo":{"status":"ok","timestamp":1608737808209,"user_tz":-540,"elapsed":342627,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"ee469ce1-8ea6-46fb-a9bb-bbf7cbd2b2de"},"source":["    # ========================================\n","    #               Testing\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Testing...\")\n","\n","    t0 = time.time()\n","    model.eval()\n","    eval_loss, eval_accuracy, eval_f1 = 0, 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in test_dataloader:\n","        batch = tuple(t.to(device).long() for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        with torch.no_grad():     \n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        logits = outputs[0]\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","     \n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        #eval_f1 += tmp_eval_f1\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Testing took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Testing complete!\")\n","\n","print(\"All Processes are completed\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Testing...\n","  Accuracy: 0.62\n","  Testing took: 0:00:06\n","\n","Testing complete!\n","All Processes are completed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tLW9YixL5D47"},"source":["# Predict\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7eetOZind1c","executionInfo":{"status":"ok","timestamp":1608737829303,"user_tz":-540,"elapsed":363716,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"602cef93-9b05-4a8f-88cd-e3c61c2b6bae"},"source":["tmp_predict_dataloader = DataLoader(predict_data, sampler=predict_sampler, batch_size=1)\n","predict_result = predict.copy(deep = True)\n","predict_result = predict_result.drop(columns = ['i_dialog', 'i_utterance', 'speaker'])\n","predict_result['Predicted'] = 'default'\n","\n","encoder = LabelEncoder()\n","labels = train['emotion'].values\n","encoder.fit(labels)\n","labels = encoder.transform(labels)\n","\n","\n","for step, batch in enumerate(tmp_predict_dataloader):\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device).long() for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_index, b_input_ids, b_input_mask = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    idx = b_index.item()\n","    predict_result['Predicted'][idx] = encoder.classes_[np.argmax(logits)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"2-LLfzDUn1az","executionInfo":{"status":"ok","timestamp":1608737829303,"user_tz":-540,"elapsed":363711,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"a37fe570-8d57-4097-c05d-83300b802f1e"},"source":["# 예측 결과 데이터 하위 5개 살펴보기\r\n","predict_result.tail()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>utterance</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1618</th>\n","      <td>1618</td>\n","      <td>Nooo.</td>\n","      <td>non-neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1619</th>\n","      <td>1619</td>\n","      <td>Hi, Kate!</td>\n","      <td>joy</td>\n","    </tr>\n","    <tr>\n","      <th>1620</th>\n","      <td>1620</td>\n","      <td>Hi, Lauren.</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1621</th>\n","      <td>1621</td>\n","      <td>Hi, Lauren.</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1622</th>\n","      <td>1622</td>\n","      <td>Hi, pig!</td>\n","      <td>joy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id    utterance    Predicted\n","1618  1618        Nooo.  non-neutral\n","1619  1619    Hi, Kate!          joy\n","1620  1620  Hi, Lauren.      neutral\n","1621  1621  Hi, Lauren.      neutral\n","1622  1622     Hi, pig!          joy"]},"metadata":{"tags":[]},"execution_count":336}]},{"cell_type":"markdown","metadata":{"id":"CNZgVsbygjwR"},"source":["# 캐글 업로드 자료 생성"]},{"cell_type":"code","metadata":{"id":"quJ08Bsy2zBg"},"source":["# 캐글 업로드를 위한 CSV 생성 (판다스 데이터 프레임 활용)\n","# 저장 경로 설정 필요\n","\n","outfile_df = pd.DataFrame()\n","\n","\n","outfile_df['Id'] = predict_result['id']\n","outfile_df['Expected'] = predict_result['Predicted']\n","\n","\n","outfile_df.to_csv('/content/drive/MyDrive/Colab Notebooks/Korea Univ/001. NLP Project/Freinds_eng/12241230.csv',index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"XiwSm1L925Qs","executionInfo":{"status":"ok","timestamp":1608737829304,"user_tz":-540,"elapsed":363704,"user":{"displayName":"손무수","photoUrl":"","userId":"06356770852246195014"}},"outputId":"ef8b7403-e813-4b1d-efae-39a8080e02ba"},"source":["outfile_df.head(4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Expected</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>surprise</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>anger</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  Expected\n","0   0   neutral\n","1   1  surprise\n","2   2     anger\n","3   3   neutral"]},"metadata":{"tags":[]},"execution_count":338}]},{"cell_type":"code","metadata":{"id":"3WPLDYb-CAgJ"},"source":[""],"execution_count":null,"outputs":[]}]}